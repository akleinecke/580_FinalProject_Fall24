{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f6dc35-a4af-4352-a83b-b834da2c5f62",
   "metadata": {},
   "source": [
    "# Training our Neural Net\n",
    "\n",
    "First, prepare training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "237525b4-853b-45c7-a4e0-981bfbfd134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000018D0A75B370>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "print (trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee2293-0794-4e86-9137-1ced2393f9f9",
   "metadata": {},
   "source": [
    "## Backpropogation\n",
    "\n",
    "NOT SURE IF CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e079ff5-690d-4008-bfb5-0f3daf5ae7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[-0.9185, -0.6129,  0.0382],\n",
      "        [-0.5686,  0.0330,  0.1796],\n",
      "        [ 1.7340,  0.5142, -0.7649]], requires_grad=True)\n",
      "y: tensor([[8.4366e-01, 3.7560e-01, 1.4577e-03],\n",
      "        [3.2326e-01, 1.0900e-03, 3.2247e-02],\n",
      "        [3.0069e+00, 2.6440e-01, 5.8501e-01]], grad_fn=<PowBackward0>)\n",
      "y.grad_fn: <PowBackward0 object at 0x0000018D09E84940>\n",
      "z: tensor(0.6037, grad_fn=<MeanBackward0>)\n",
      "x.grad: tensor([[-0.2041, -0.1362,  0.0085],\n",
      "        [-0.1263,  0.0073,  0.0399],\n",
      "        [ 0.3853,  0.1143, -0.1700]])\n",
      "2*x/9: tensor([[-0.2041, -0.1362,  0.0085],\n",
      "        [-0.1263,  0.0073,  0.0399],\n",
      "        [ 0.3853,  0.1143, -0.1700]], grad_fn=<DivBackward0>)\n",
      "y.grad: tensor([[0.1111, 0.1111, 0.1111],\n",
      "        [0.1111, 0.1111, 0.1111],\n",
      "        [0.1111, 0.1111, 0.1111]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,3, requires_grad=True)\n",
    "print(\"x:\", x)\n",
    "\n",
    "y = x**2\n",
    "print(\"y:\", y)\n",
    "\n",
    "## grad_fn shows the function that generated this variable\n",
    "print(\"y.grad_fn:\", y.grad_fn)\n",
    "\n",
    "y.retain_grad()\n",
    "\n",
    "z = y.mean()\n",
    "print(\"z:\", z)\n",
    "\n",
    "z.backward()\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"2*x/9:\", 2*x/9)\n",
    "print(\"y.grad:\", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a824d-6f91-44a6-b5fd-a6be5d217798",
   "metadata": {},
   "source": [
    "## Loss and Autograd together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9863c8b3-0a52-4d24-8ff4-a926b8889a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 2.6610e-05,  2.6610e-05,  2.6610e-05,  ...,  2.6610e-05,\n",
      "          2.6610e-05,  2.6610e-05],\n",
      "        [ 4.8405e-04,  4.8405e-04,  4.8405e-04,  ...,  4.8405e-04,\n",
      "          4.8405e-04,  4.8405e-04],\n",
      "        [ 4.1136e-04,  4.1136e-04,  4.1136e-04,  ...,  4.1136e-04,\n",
      "          4.1136e-04,  4.1136e-04],\n",
      "        ...,\n",
      "        [-1.5820e-03, -1.5820e-03, -1.5820e-03,  ..., -1.5820e-03,\n",
      "         -1.5820e-03, -1.5820e-03],\n",
      "        [ 3.8668e-03,  3.8668e-03,  3.8668e-03,  ...,  3.8668e-03,\n",
      "          3.8668e-03,  3.8668e-03],\n",
      "        [ 9.1510e-04,  9.1510e-04,  9.1510e-04,  ...,  9.1510e-04,\n",
      "          9.1510e-04,  9.1510e-04]])\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128), # Flattened MNIST image sizes (28x28) & Linear layer maps\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10), # 10 output layers for each digit\n",
    "                      nn.LogSoftmax(dim=1))  # this line is extra comparing to earlier nn.Sequential calls\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model(images)\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f11f9f-5e5f-4dbb-a96f-cb6261e8c0b1",
   "metadata": {},
   "source": [
    "## Training the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b3c6f44-eb5c-4988-aea1-99a97cdae03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x0000018D0A745510>\n",
      "torch.Size([128, 784])\n",
      "torch.Size([128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "print(model.parameters())\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "165edcc8-522d-4195-b67e-5c62d748c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-0.0164, -0.0335,  0.0185,  ..., -0.0261,  0.0184, -0.0218],\n",
      "        [ 0.0349, -0.0225, -0.0349,  ...,  0.0076,  0.0175,  0.0006],\n",
      "        [-0.0347,  0.0137, -0.0325,  ...,  0.0286,  0.0035, -0.0252],\n",
      "        ...,\n",
      "        [-0.0282,  0.0017, -0.0025,  ..., -0.0073,  0.0283,  0.0160],\n",
      "        [ 0.0292,  0.0062,  0.0017,  ...,  0.0139,  0.0218,  0.0102],\n",
      "        [ 0.0330, -0.0005,  0.0019,  ...,  0.0161,  0.0098, -0.0028]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[ 0.0012,  0.0012,  0.0012,  ...,  0.0012,  0.0012,  0.0012],\n",
      "        [-0.0010, -0.0010, -0.0010,  ..., -0.0010, -0.0010, -0.0010],\n",
      "        [ 0.0016,  0.0016,  0.0016,  ...,  0.0016,  0.0016,  0.0016],\n",
      "        ...,\n",
      "        [-0.0058, -0.0058, -0.0058,  ..., -0.0058, -0.0058, -0.0058],\n",
      "        [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
      "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002]])\n",
      "Updated weights -  Parameter containing:\n",
      "tensor([[-0.0164, -0.0335,  0.0185,  ..., -0.0261,  0.0183, -0.0218],\n",
      "        [ 0.0349, -0.0225, -0.0349,  ...,  0.0076,  0.0175,  0.0006],\n",
      "        [-0.0347,  0.0137, -0.0325,  ...,  0.0286,  0.0034, -0.0252],\n",
      "        ...,\n",
      "        [-0.0281,  0.0018, -0.0024,  ..., -0.0073,  0.0283,  0.0161],\n",
      "        [ 0.0292,  0.0062,  0.0017,  ...,  0.0139,  0.0218,  0.0102],\n",
      "        [ 0.0330, -0.0005,  0.0019,  ...,  0.0161,  0.0098, -0.0028]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784) # another way to flatten\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model[0].weight.grad)\n",
    "\n",
    "# Take an update step and few the new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d365af-8b65-4eca-b6a6-005f09c71c22",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "Now we'll put this algorithm into a loop so we can go through all the images. Some nomenclature, one pass through the entire dataset is called an *epoch*. So here we're going to loop through `trainloader` to get our training batches. For each batch, we'll doing a training pass where we calculate the loss, do a backwards pass, and update the weights.\n",
    "\n",
    ">**Final Project:** This is the training pass for our network. If implemented correctly, you should see the training loss drop with each epoch.\n",
    "\n",
    "1. Training Pass, calculate loss\n",
    "2. Backwards Pass\n",
    "3. Update Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9230da8-f830-4ccc-8479-0e921f1459f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.2557\n",
      "Epoch 2/5, Loss: 0.2362\n"
     ]
    }
   ],
   "source": [
    "# Steps:\n",
    "# Clear the gradients\n",
    "# Forward pass\n",
    "# Calc loss\n",
    "# Back pass to computer grads\n",
    "# Update model\n",
    "\n",
    "\n",
    "epochs = 5 # Number of training cycles\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.size(0), -1) # Flatten our images to 784 (28x28)\n",
    "\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(images)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127beb2e-85be-4617-a36c-476c0ad086e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.arange(938)\n",
    "print (f\"Length of trainloader:{len(trainloader)}\")\n",
    "nplosses = np.array(losses)\n",
    "plt.figure(figsize=[12, 6])\n",
    "plt.plot(x, losses[0], color='yellow', label=\"1st\")\n",
    "plt.plot(x, losses[1], color='b', label=\"2nd\")\n",
    "plt.plot(x, losses[2], color='r', label=\"3rd\")\n",
    "plt.plot(x, losses[3], color='g', label=\"4th\")\n",
    "plt.plot(x, losses[4], color='orange', label=\"5th\")\n",
    "plt.xlabel('batch idx')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
